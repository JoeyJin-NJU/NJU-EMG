<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<base href="https://emg-nju.github.io/Homepage/">
<link rel="stylesheet" href="css/style.css">
<title>新闻 | 南京大学工程医学研究组</title>
<script src="header-news.js"></script>
</head>

<body>
<div id="header-news"></div>
<div>
  <table width="100%" border="0" cellspacing="0" cellpadding="0">
    <tr>
      <td>&nbsp;</td>
    </tr>
    <tr>
	  <td colspan="3" align="center" style="font-family: '微软雅黑'; color: #000000; font-size: 24px;"><strong>研究组周游赴意大利米兰参加2024欧洲计算机视觉会议（ECCV 2024）</strong></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
    </tr>

  </table>
</div>
<div>
  <table width="100%" border="0" cellspacing="10px" cellpadding="0">
    <tr>
      <td width="30">&nbsp;</td>
      <td colspan="3" class="author1" align="center" style="font-size: 20px"><strong>时间：2024/10/10&nbsp;&nbsp;&nbsp;&nbsp;作者：周游、江斌</strong></td>
      <td width="30">&nbsp;</td>
    </tr>
    <tr>
      <td rowspan="30">&nbsp;</td>
      <td colspan="3" class="Context1" style="font-size: 20px">2024年9月29日至10月4日，计算机视觉和机器学习领域的顶级国际会议之一ECCV (European Conference on Computer Vision）在意大利北部城市米兰（Milan）的召开，研究组周游老师赴意大利参会。</td>
      <td rowspan="15">&nbsp;</td>
    </tr>
	  
	<tr>
      <td colspan="3"><p style="text-align: center"><img src="images/AttendingMeeting34-1.jpg" width="700" height="524" alt=""/></p>
		   <p class="Context1" style="text-align: center; font-size: 20px;">意大利米兰ECCV会场外观</p>
    </td>
    </tr>
	  
	<tr>
      <td colspan="3">&nbsp;</td>
    </tr> 
	  
	<tr>
      <td colspan="3"><p style="text-align: center"><img src="images/AttendingMeeting34-2.jpg" width="700" height="524" alt=""/></p>
		  <p class="Context1" style="text-align: center; font-size: 20px;">南京大学电子学院江斌博士参加墙壁展示环节，同与会专家学术探讨</p>
    </td>
    </tr>
	  
    <tr>
      <td colspan="3">&nbsp;</td>
    </tr> 
	  
	<tr>
	  <td colspan="3" class="Context1" style="font-size: 20px">研究组周游老师与南京大学电子学院马展老师课题组合作，投稿《EDformer: Transformer-Based Event Denoising Across Varied Noise Levels》，被选为墙报展示论文。南京大学电子学院的江斌博士和北京大学计算机学院的熊博博士是论文的共同第一作者。本工作还得到了加州大学河滨分校M. Salman Asif教授的指导，以及南京大学电子学院屈泊含同学在实验设计和数据采集上的支持。文章提出EDformer——一种用于事件相机（Event Camera）多噪声水平去噪的创新模型。当前研究针对事件相机在不同亮度条件下的非信息性背景活动噪声的关注较少，相关的真实世界数据集也极为稀缺，这一缺陷导致现有事件去噪算法在实际场景中缺乏稳健性。为解决这一问题，本文通过DAVIS346事件相机在不同光照条件下收集和分析了背景活动噪声，并引入了首个真实世界的事件去噪数据集 ED24，其中包含21种噪声水平及噪声注释；此外，研究团队提出了EDformer，这是一种基于Transformer的事件逐点去噪模型，能够学习事件之间的时空相关性，从而在多种噪声水平下实现出色的去噪效果。与现有去噪算法相比，EDformer 在去噪精度上达到了最新的水平，适用于包括开源数据集以及实际低光强场景（如显微成像场景：斑马鱼血管成像等）的多种应用。</td>
    </tr>  
	  
	<tr>
      <td colspan="3"><p style="text-align: center"><img src="images/AttendingMeeting34-3.jpg" width="700" height="261" alt=""/></p>
		  <p class="Context1" style="text-align: center; font-size: 20px;">研究组周游老师、北京大学熊博老师参会</p>
    </td>
    </tr>
	  
	<tr>
      <td colspan="3">&nbsp;</td>
    </tr> 
	  
	<tr>
	  <td colspan="3" class="Context1" style="font-size: 20px">欧洲计算机视觉会议（ECCV）由欧洲计算机视觉协会（ECVA）管理，每偶数年举行一次，聚集计算机视觉和机器学习相关领域的科学界和工业界专家、学者、以及在校学生等。会议期间，来自世界各地的研究者汇报团队近期的研究成果，下面对相关方向的研究进展进行概述，主要聚焦事件相机方向。</td>
    </tr>    
	  
	<tr>
      <td colspan="3"><p style="text-align: center"><img src="images/AttendingMeeting34-4.jpg" width="700" height="261" alt=""/></p>
    </td>
    </tr>  
	  
	<tr>
	  <td colspan="3" class="Context1" style="font-size: 20px">来自苏黎世联邦理工的Valery Vishnevskiy等研究人员在其论文《Optimal OnTheFly Feedback Control of Event Sensors》中提出了一种名为“OnTheFly”的创新技术，通过动态调整事件传感器的激活阈值来优化图像重建。这种技术能够基于用户定义的目标事件率、以智能方式分析并调整传感器的激活参数，从而显著提高图像重建质量。实验结果表明，与传统方法相比，该技术有效平衡了图像质量和传感器事件率，体现了其在机器人、计算机视觉等领域的广泛应用潜力。</td>
    </tr> 
	
	<tr>
      <td colspan="3"><p style="text-align: center"><img src="images/AttendingMeeting34-5.jpg" width="700" height="528" alt=""/></p>
    </td>
    </tr>
	
	<tr>
	  <td colspan="3" class="Context1" style="font-size: 20px"><p>——在《CEIA: CLIP-Based Event-Image Alignment for Open-World Event-Based Understanding》一文中，来自中科大的Wenhao Xu 等研究人员提出了 CEIA 框架，以解决事件数据与文本数据配对稀缺的问题，推动事件数据在开放世界中的理解。CEIA 通过对比学习，将事件数据和图像数据对齐，以替代直接对齐事件与文本数据。利用丰富的事件-图像数据集，CEIA 通过 CLIP 的图像空间创建与事件空间的嵌入对齐，从而借助图像数据作为桥梁，实现事件和文本数据的自然对齐。实验评估表明，CEIA 在对象识别、事件-图像检索、事件-文本检索及领域适应等多种多模态应用中，具备显著的零样本优势。</p>
        <p>——在论文《ES-PTAM: Event-based Stereo Parallel Tracking and Mapping》中，来自柏林工业大学的Suman Ghosh等人 提出了一种新颖的事件双目视觉里程计系统。尽管视觉里程计（VO）和SLAM技术已取得显著进展，但目前的系统仍受到传感器能力的限制。事件相机作为一种新型视觉传感器，能够克服传统相机在高动态范围和高速运动场景中的局限性。ES-PTAM 结合了两大创新：一个基于射线密度融合的无对应点映射模块，用于深度估计；以及一个基于边缘图对齐的跟踪模块，用于摄像机位姿估计。通过在五个实际数据集上的广泛评估（涵盖多种相机类型和应用场景，如驾驶、无人机、手持设备、第一人称视角等），实验结果表明，ES-PTAM 在大多数测试序列上显著优于现有方法。</p>
        <p>——在论文《Event-Based Fusion for Motion Deblurring with Cross-modal Attention》中，来自武汉大学的Weiqi Luo 等研究人员利用事件相机与图像相机的跨模态特性，提出了一种创新的运动去模糊方法。传统相机因长曝光时间而不可避免地产生运动模糊，而事件相机以高时间分辨率记录亮度变化，为图像退化信息的捕捉提供了新方式。研究团队提出了一种端到端的两阶段图像修复网络EFNet，通过事件-图像跨模态注意力模块在多个层级融合事件与图像特征，聚焦于事件分支中的有效信息并过滤噪声。此外，团队还设计了一种对称累积事件表示和事件掩模门控连接，进一步减少信息损失，提升图像去模糊效果。为了促进事件相机去模糊研究，研究人员构建了一个新数据集REBlur，专门在受控光学实验室中捕捉极端模糊场景。实验结果显示，EFNet在GoPro和REBlur数据集上刷新了运动去模糊的最新标准，特别在极端模糊条件下表现尤为出色。</p>
      <p>——在论文《Lossless Encoding of Time-Aggregated Neuromorphic Vision Sensor Data Based on Point-Cloud Compression》中，Jayasingam Adhuran等人探讨了神经形态视觉传感器（NVS）的数据压缩问题。NVS 以异步方式捕获视觉信息，在场景发生变化时触发数据采集，相较于同步捕获（基于帧的视频），其优势包括低功耗、高动态范围、极高的时间分辨率和较低的数据传输率。尽管NVS的采集策略已经使得数据率大幅降低，仍然有进一步压缩的空间。为此，研究团队提出了基于时间聚合的无损视频编码方法（TALVEN），该方法将NVS事件以像素为基础的事件直方图形式进行时间聚合，以特定格式安排数据，并采用了来源于视频编码的无损压缩技术进行实现。</p></td>
    </tr> 
	
	<tr>
      <td colspan="3"><p style="text-align: center"><img src="images/AttendingMeeting34-6.jpg" width="700" height="551" alt=""/></p>
    </td>
    </tr>
	  
	<tr>
	  <td colspan="3" class="Context1" style="font-size: 20px"><p>——在论文《EventSleep: Sleep Activity Recognition with Event Cameras》中，来自萨拉戈萨大学的Carlos Plou 等研究人员介绍了EventSleep，这是一种利用事件相机进行睡眠活动识别的新方法与数据集。由于事件相机在低光环境下的独特优势，使其在活动识别方面具有极大潜力。然而，目前低光条件下的事件相机数据集较为稀缺，限制了该技术在暗环境下的应用。EventSleep数据集包含同步的事件和红外记录，模拟睡眠期间常见的运动，为暗环境下的活动识别提供了一个独特且富有挑战性的新数据集。研究团队提出了一种新的分析管道，采用贝叶斯方法（Laplace集成）来增强预测的鲁棒性，这是医学应用中至关重要的因素。该研究是首次将贝叶斯神经网络应用于事件相机，并在实际问题中使用Laplace集成。实验结果不仅验证了事件相机在暗环境中监测睡眠活动的潜力，也表明了其在睡眠活动识别中的高准确性与稳健性，同时为事件数据预处理技术在暗环境中的应用打开了新的研究方向。</p>
        <p>——在论文《Minimalist Vision with Freeform Pixels》中，来自哥伦比亚大学的Jeremy Klotz 和 Shree Nayar 提出了极简视觉系统的概念，该系统通过使用最少的像素数量来完成视觉任务。与传统相机采用大量方形像素阵列不同，极简相机使用可以任意形状的自由形状像素，从而提高信息含量。研究表明，极简相机的硬件可以被建模为神经网络的第一层，而后续层则用于推理。通过为特定任务训练网络，可以得到相机自由形状像素的形状，每个像素由光电探测器和光学掩模实现。研究团队设计了用于室内空间监测、室内照明测量和交通流量估计的极简相机，每个相机均配备8个像素。这些系统的性能与传统相机相比，尽管像素数量相差几个数量级，却表现相当。极简视觉系统具有两个主要优势：首先，它自然地保护了场景中个体的隐私，因为捕获的信息不足以提取出详细的视觉信息；其次，由于极简相机进行的测量非常少，研究表明该相机可以完全自供电，即无需外部电源或电池即可正常工作。</p>
        <p>——在论文《A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging》中，来自西湖大学的Miao Cao等人提出了一个简单的低比特量化框架（称为 Q-SCI），旨在提高视频快照压缩成像（SCI）中基于深度学习的方法的效率。视频快照压缩成像旨在利用低速二维相机捕捉高速度场景作为快照压缩测量，然后通过重建算法重建高速视频帧。</p>
      <p>——在论文《Flying with Photons: Rendering Novel Views of Propagating Light》中，来自多伦多大学的Anagh Malik等研究者提出了一种成像和神经渲染技术，旨在通过移动摄像机视角来合成光在场景中传播的视频。该方法依赖于一种全新的超快速成像设备，捕捉到了首个具有皮秒级时间分辨率的多视角视频数据集。结合这一数据集，研究团队引入了一种基于瞬态场的高效神经体积渲染框架。瞬态场被定义为从三维点和二维方向映射到高维离散时间信号的过程，该信号表示在超快速时间尺度上变化的辐射亮度。使用瞬态场进行渲染时，自然考虑了光速有限引起的效应，包括因光传播延迟导致的视角相关外观变化。研究中渲染了多种复杂效果，包括散射、镜面反射、折射和衍射。此外，团队展示了使用时间扭曲过程消除视角相关传播延迟、相对论效应的渲染、以及光传输的直接和全局成分的视频合成。</p></td>
    </tr> 
	  
	<tr>
      <td colspan="3"><p style="text-align: center"><img src="images/AttendingMeeting34-7.jpg" width="700" height="262" alt=""/></p>
    </td>
    </tr>
	  
	<tr>
	  <td colspan="3" class="Context1" style="font-size: 20px"><p>——在论文《SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow》中，来自普林斯顿大学的Yihan Wang等人提出了SEA-RAFT，这是一种更简单、高效且精确的光流估计方法。与RAFT相比，SEA-RAFT采用了一种新的损失函数（混合拉普拉斯），并直接回归初始光流，从而加快迭代优化的收敛速度。此外，该方法引入了刚性运动的预训练，以提升模型的泛化能力。SEA-RAFT在Spring基准测试上达到了最先进的准确度；在KITTI和Spring数据集上展现了最佳的跨数据集泛化能力。凭借其高效性，SEA-RAFT的运行速度至少比现有方法快2.3倍，同时保持了具有竞争力的性能。</p>
      <p>——在论文《GazeXplain: Learning to Predict Natural Language Explanations of Visual Scanpaths》中，来自明尼苏达大学的Xianyu Chen等人提出了GazeXplain，一种能够预测和解释人类视觉扫描路径的新方法。人类在观察视觉场景时，其扫描路径受注意力过程驱动。理解视觉扫描路径对于许多应用至关重要。然而，传统的扫描路径模型只能预测视线的“何处”和“何时”，而无法解释注视的原因，从而形成了理解注视背后的理性空白。</p></td>
    </tr>
	
	  
	<tr>
      <td colspan="3">&nbsp;</td>
    </tr>

	  
  </table>
</div>
<p>&nbsp;</p>
</body>
</html>
